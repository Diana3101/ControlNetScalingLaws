{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd016a0-db85-4f28-b6e6-8288d3334b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.fields.decoders import SimpleRGBImageDecoder, BytesDecoder\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, RandomTranslate, Convert, ToTensor, ToDevice, ToTorchImage\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.fields import RGBImageField, JSONField, BytesField\n",
    "import torchvision.transforms\n",
    "from transformers.image_transforms import center_to_corners_format\n",
    "\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566df68c-72ff-47ec-9dff-8e40ea348a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from dataclasses import replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4e7c39-1b38-409a-905b-2ccd33926a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from glob import glob\n",
    "from os import path\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0896c07b-e4c4-4816-8364-662b2d33a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb import AlertLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c19525-5f57-460a-9f61-90973b8f6bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiana15kapatsyn\u001b[0m (\u001b[33mteam__1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e44f9260-2d31-432d-8fe3-e17c86665689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7629f8bb-e1a8-4222-976a-65876020e5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import features\n",
    "features.check_feature(\"libjpeg_turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5984e1-28f1-4c36-a2d0-6983fa370660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c72c26-239d-44b1-9a10-f207ae9f4054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab852e1-ec21-4f02-aa2c-dc116b5576d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3a2e4d3-a535-4850-b6c3-d5072bb19fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truly random shuffling (shuffle=True in PyTorch)\n",
    "# ORDERING = OrderOption.RANDOM\n",
    "\n",
    "# Unshuffled (i.e., served in the order the dataset was written)\n",
    "ORDERING = OrderOption.SEQUENTIAL\n",
    "\n",
    "# Memory-efficient but not truly random loading\n",
    "# Speeds up loading over RANDOM when the whole dataset does not fit in RAM!\n",
    "# ORDERING = OrderOption.QUASI_RANDOM\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 12\n",
    "IMAGE_RESOLUTION = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e4d00b-3ba3-4c6e-99f4-85fe530b797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Rescale, self).__init__()\n",
    "        self.scale = 255.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d454e1a-afc0-4b40-86ec-00a4348f857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mean = [\n",
    "    0.485,\n",
    "    0.456,\n",
    "    0.406\n",
    "  ]\n",
    "\n",
    "image_std = [\n",
    "    0.229,\n",
    "    0.224,\n",
    "    0.225\n",
    "  ]\n",
    "\n",
    "image_pipeline: List[Operation] = [SimpleRGBImageDecoder(), ToTensor(), Convert(torch.float16), Rescale(), \n",
    "                                   ToDevice(torch.device('cuda'), non_blocking=True), ToTorchImage(),\n",
    "                                   torchvision.transforms.Normalize(image_mean, image_std)]\n",
    "\n",
    "PIPELINES = {\n",
    "  'image': image_pipeline\n",
    "}\n",
    "\n",
    "loader = Loader('/shared_drive/user-files/laion_dataset_200M/ffcv-laion200m-1shard.beton', \n",
    "                batch_size=BATCH_SIZE,\n",
    "                num_workers=NUM_WORKERS,\n",
    "                order=ORDERING,\n",
    "                pipelines=PIPELINES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51e99a35-fbd9-4f47-96d6-2ad6bc141de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517eb128-4d32-447a-a1e6-a1d9655d851f",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eec1c70-df07-407c-80e6-ea14b2cedfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "\n",
    "model = model.to(device, dtype=torch.float16)\n",
    "\n",
    "target_sizes = torch.tensor([[IMAGE_RESOLUTION, IMAGE_RESOLUTION]]*BATCH_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "793df8ea-e300-4b39-947a-9f1400bb5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_parquet(results, urls, model, step):\n",
    "    rows = []\n",
    "    columns = ['url', 'label', 'score', 'top_left_x', 'top_left_y', 'bottom_right_x', 'bottom_right_y']\n",
    "    for i, result_per_image in enumerate(results):\n",
    "        for score, label, box in zip(result_per_image[\"scores\"], result_per_image[\"labels\"], result_per_image[\"boxes\"]):\n",
    "            url = urls[i]\n",
    "            box = box.detach().cpu()\n",
    "            top_left_x, top_left_y, bottom_right_x, bottom_right_y = box[0].item(), box[1].item(), box[2].item(), box[3].item()\n",
    "            label_name = model.config.id2label[label.item()]\n",
    "            score = np.round(score.detach().cpu().item(), 2)\n",
    "    \n",
    "            row = [url, label_name, score, top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n",
    "            rows.append(row)\n",
    "        \n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df.to_parquet(f'/mnt/disks/disk-big2/laion200m-od-labels-1shard/{step}_batch.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2038e00e-be40-4930-a6a9-7e7671b98387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_object_detection(\n",
    "        outputs, threshold: float = 0.5, target_sizes = None\n",
    "    ):\n",
    "        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n",
    "\n",
    "        if target_sizes is not None:\n",
    "            if len(out_logits) != len(target_sizes):\n",
    "                raise ValueError(\n",
    "                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n",
    "                )\n",
    "\n",
    "        # t = time()\n",
    "        prob = nn.functional.softmax(out_logits, -1)\n",
    "        scores, labels = prob[..., :-1].max(-1)\n",
    "        # print(time() - t)\n",
    "\n",
    "        # t = time()\n",
    "        # Convert to [x0, y0, x1, y1] format\n",
    "        boxes = center_to_corners_format(out_bbox)\n",
    "        # print(time() - t)\n",
    "\n",
    "        mask = scores > threshold\n",
    "    \n",
    "        # mask = mask.cpu()\n",
    "        # scores = scores.cpu()\n",
    "        # labels = labels.cpu()\n",
    "        # boxes = boxes.cpu()\n",
    "\n",
    "        # t = time()\n",
    "        # Convert from relative [0, 1] to absolute [0, height] coordinates\n",
    "        if target_sizes is not None:\n",
    "            if isinstance(target_sizes, List):\n",
    "                img_h = torch.Tensor([i[0] for i in target_sizes])\n",
    "                img_w = torch.Tensor([i[1] for i in target_sizes])\n",
    "            else:\n",
    "                img_h, img_w = target_sizes.unbind(1)\n",
    "\n",
    "            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n",
    "            boxes = boxes * scale_fct[:, None, :]\n",
    "        # print(time() - t)\n",
    "\n",
    "        # t = time()\n",
    "        results = []\n",
    "    \n",
    "        # print(len(scores))\n",
    "        # mask_idx, class_idx = torch.where(mask)\n",
    "        # class_idx[i, i*128]\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "        # for s, l, b in zip(scores, labels, boxes):\n",
    "            score = scores[i][mask[i]]\n",
    "            label = labels[i][mask[i]]\n",
    "            box = boxes[i][mask[i]]\n",
    "            # print(s.shape)\n",
    "            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n",
    "        # print(time() - t)\n",
    "        # print()\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5c4bb81-a0a9-4a48-8413-6796d0640898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = torch.rand(128, 100)\n",
    "# mask = scores > 0.5\n",
    "\n",
    "# coord = torch.nonzero(mask)\n",
    "# torch.where(mask), torch.where(mask)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ecc95-2aee-4dc9-a417-42a24e2e2d65",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269d9cf8-fe25-4505-a385-952cee5c4ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/shared_drive/user-files/ControlNetScalingLaws/wandb/run-20240316_204354-gq58fnpr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/team__1/object-detector/runs/gq58fnpr' target=\"_blank\">ancient-shadow-4</a></strong> to <a href='https://wandb.ai/team__1/object-detector' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/team__1/object-detector' target=\"_blank\">https://wandb.ai/team__1/object-detector</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/team__1/object-detector/runs/gq58fnpr' target=\"_blank\">https://wandb.ai/team__1/object-detector/runs/gq58fnpr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"object-detector\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de981c4f-0efc-4d36-8c76-b605fe376482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e01664764f406a9f6ab801740366ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "for step, (batch, label) in tqdm(enumerate(loader)):\n",
    "    try:\n",
    "        label_json = JSONField.unpack(label)\n",
    "        \n",
    "        batch_urls = [dict_['url'] for dict_ in label_json]\n",
    "            \n",
    "        batch_dict = {'pixel_values': batch}\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch_dict)\n",
    "        \n",
    "        results = post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)\n",
    "        \n",
    "        save_results_to_parquet(results=results, \n",
    "                                    urls=batch_urls, \n",
    "                                    model=model, \n",
    "                                    step=step)\n",
    "        wandb.log({\"n_batch\": step})\n",
    "    except:\n",
    "        wandb.alert(title=f\"Batch Warning!\",\n",
    "                    text=f\"Problem with batch {step}\",\n",
    "                    level=AlertLevel.WARN)\n",
    "\n",
    "print(time() - t)\n",
    "wandb.alert(title=f\"Run finished!\",\n",
    "            text = f\"Objects successfully detected in all {step+1} batches in {np.round((time() - t)/3600, 2)} hours !!! :)\",\n",
    "                   level=AlertLevel.INFO)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c437051-5ab5-412e-a7ba-6a24a868dde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9cee945-a2f2-499e-a296-e743df8635fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/mnt/disks/disk-big2/laion200m-od-labels-1shard/0_batch.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf2e4f22-8380-4b6f-b03d-c06d05b1b106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj6BI/AAAAAAAAasE/uT9KjqByvLQ/s330/%25E8%25A8%2598%25E4%25BA%258B%25E6%259C%25AC_121214_193308_1.jpg'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e57cbff-59fe-40a0-a11c-66936b8ff408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>top_left_x</th>\n",
       "      <th>top_left_y</th>\n",
       "      <th>bottom_right_x</th>\n",
       "      <th>bottom_right_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://t0.gstatic.com/images?q=tbn:ANd9GcQsZJi...</td>\n",
       "      <td>car</td>\n",
       "      <td>1.00</td>\n",
       "      <td>56.656250</td>\n",
       "      <td>137.5000</td>\n",
       "      <td>152.00000</td>\n",
       "      <td>164.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj...</td>\n",
       "      <td>clock</td>\n",
       "      <td>1.00</td>\n",
       "      <td>118.812500</td>\n",
       "      <td>141.8750</td>\n",
       "      <td>140.12500</td>\n",
       "      <td>160.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj...</td>\n",
       "      <td>person</td>\n",
       "      <td>0.90</td>\n",
       "      <td>124.875000</td>\n",
       "      <td>221.1250</td>\n",
       "      <td>137.87500</td>\n",
       "      <td>246.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://st.depositphotos.com/3336339/4632/i/170...</td>\n",
       "      <td>tv</td>\n",
       "      <td>0.99</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>21.0625</td>\n",
       "      <td>230.50000</td>\n",
       "      <td>205.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://img.deanscards.com/thumb/1357368b.jpg</td>\n",
       "      <td>book</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>43.2500</td>\n",
       "      <td>255.37500</td>\n",
       "      <td>219.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://cdn11.bigcommerce.com/s-1mxugrbmxo/ima...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>1.00</td>\n",
       "      <td>21.875000</td>\n",
       "      <td>39.6875</td>\n",
       "      <td>236.12500</td>\n",
       "      <td>221.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>http://v.fashionov.com/products/small/122421/2...</td>\n",
       "      <td>person</td>\n",
       "      <td>1.00</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>151.7500</td>\n",
       "      <td>230.75000</td>\n",
       "      <td>249.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>http://media.rightmove.co.uk/dir/10k/9111/5323...</td>\n",
       "      <td>car</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>173.3750</td>\n",
       "      <td>31.46875</td>\n",
       "      <td>226.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>http://media.rightmove.co.uk/dir/10k/9111/5323...</td>\n",
       "      <td>car</td>\n",
       "      <td>0.99</td>\n",
       "      <td>193.500000</td>\n",
       "      <td>168.2500</td>\n",
       "      <td>256.00000</td>\n",
       "      <td>230.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>https://african-import.com/wp-content/uploads/...</td>\n",
       "      <td>person</td>\n",
       "      <td>1.00</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>39.0000</td>\n",
       "      <td>230.00000</td>\n",
       "      <td>256.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url   label  score  \\\n",
       "0    http://t0.gstatic.com/images?q=tbn:ANd9GcQsZJi...     car   1.00   \n",
       "1    http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj...   clock   1.00   \n",
       "2    http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj...  person   0.90   \n",
       "3    http://st.depositphotos.com/3336339/4632/i/170...      tv   0.99   \n",
       "4         http://img.deanscards.com/thumb/1357368b.jpg    book   0.90   \n",
       "..                                                 ...     ...    ...   \n",
       "168  https://cdn11.bigcommerce.com/s-1mxugrbmxo/ima...  laptop   1.00   \n",
       "169  http://v.fashionov.com/products/small/122421/2...  person   1.00   \n",
       "170  http://media.rightmove.co.uk/dir/10k/9111/5323...     car   0.99   \n",
       "171  http://media.rightmove.co.uk/dir/10k/9111/5323...     car   0.99   \n",
       "172  https://african-import.com/wp-content/uploads/...  person   1.00   \n",
       "\n",
       "     top_left_x  top_left_y  bottom_right_x  bottom_right_y  \n",
       "0     56.656250    137.5000       152.00000         164.250  \n",
       "1    118.812500    141.8750       140.12500         160.875  \n",
       "2    124.875000    221.1250       137.87500         246.875  \n",
       "3     18.000000     21.0625       230.50000         205.250  \n",
       "4      0.875000     43.2500       255.37500         219.250  \n",
       "..          ...         ...             ...             ...  \n",
       "168   21.875000     39.6875       236.12500         221.750  \n",
       "169  175.000000    151.7500       230.75000         249.750  \n",
       "170    0.023438    173.3750        31.46875         226.375  \n",
       "171  193.500000    168.2500       256.00000         230.000  \n",
       "172   57.000000     39.0000       230.00000         256.000  \n",
       "\n",
       "[173 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d618ff-5e78-40fd-9e30-fc42264489ae",
   "metadata": {},
   "source": [
    "10 batches (128 images) - 1 sec (FFCV loader)\\\n",
    "5 batch (128 images) - 1 sec (HF loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e12018-acd7-4928-af6b-fd59ba66a4ba",
   "metadata": {},
   "source": [
    "# HF dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea20c7a-4287-4241-b3ca-bf05b427895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79804925-d565-41d4-a070-dc246e39ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = '/shared_drive/user-files/laion_dataset_200M/laion200m-data/0-12_2M/'\n",
    "\n",
    "my_shards = glob(path.join(FOLDER, '*'))\n",
    "tar_my_shards = [file for file in my_shards if file.endswith('.tar')][:1]\n",
    "\n",
    "seed=42 \n",
    "buffer_size=100\n",
    "\n",
    "iterable_dataset = load_dataset(\"webdataset\", data_files={\"train\": tar_my_shards}, split=\"train\", \n",
    "                       streaming=True)\n",
    "\n",
    "def get_url(example):\n",
    "        url = example['json']['url']\n",
    "        example['url'] = url\n",
    "        return example\n",
    "    \n",
    "iterable_dataset = iterable_dataset.map(get_url)\n",
    "train_dataset = iterable_dataset\n",
    "# .shuffle(seed=seed, buffer_size=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91f42d3f-2cd7-48af-b626-9a6df099f9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/shared_drive/user-files/laion_dataset_200M/laion200m-data/0-12_2M/00682.tar']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_my_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c7cc344-b178-4356-bd47-4516ba3a6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(train_dataset): \n",
    "    images = []\n",
    "    urls = []\n",
    "    \n",
    "    for example in train_dataset:\n",
    "        images.append(example['jpg'].convert(\"RGB\"))\n",
    "        urls.append(example['url'])\n",
    "\n",
    "    processed = detr_processor(images=images, return_tensors=\"pt\", do_resize=False)\n",
    "    # print(detr_processor)\n",
    "    return processed, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c08685c-eca0-4802-b709-d958b67a7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "dataloader = DataLoader(train_dataset, \n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b866ae09-441c-42c4-b9d2-b48688787f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c3fcb05e2541c9b826a0c9d1d5bfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1469033/170280622.py\u001b[0m(2)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 2 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m    \u001b[0;31m# print(batch['pixel_values'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m    \u001b[0mbatch_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0mbatch_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256, 256])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][0].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][0].min()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][0].max()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][10].min()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'].min()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'].max()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "for step, batch in tqdm(enumerate(dataloader)):\n",
    "    # print(batch['pixel_values'].shape)\n",
    "    batch_urls = batch[1]\n",
    "    batch_tensors = batch[0]\n",
    "    pdb.set_trace()\n",
    "    \n",
    "print(time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c7808-55e1-4bdc-9204-0f7301ca182c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059104c-cf07-46d3-a8ec-4228408aca34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

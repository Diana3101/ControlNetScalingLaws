{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd016a0-db85-4f28-b6e6-8288d3334b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.fields.decoders import SimpleRGBImageDecoder, BytesDecoder\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, RandomTranslate, Convert, ToTensor, ToDevice, ToTorchImage\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.fields import RGBImageField, JSONField, BytesField\n",
    "import torchvision.transforms\n",
    "from transformers.image_transforms import center_to_corners_format\n",
    "\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566df68c-72ff-47ec-9dff-8e40ea348a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from dataclasses import replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4e7c39-1b38-409a-905b-2ccd33926a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from glob import glob\n",
    "from os import path\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0896c07b-e4c4-4816-8364-662b2d33a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb import AlertLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c19525-5f57-460a-9f61-90973b8f6bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiana15kapatsyn\u001b[0m (\u001b[33mteam__1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44f9260-2d31-432d-8fe3-e17c86665689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7629f8bb-e1a8-4222-976a-65876020e5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import features\n",
    "features.check_feature(\"libjpeg_turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f5984e1-28f1-4c36-a2d0-6983fa370660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c72c26-239d-44b1-9a10-f207ae9f4054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab852e1-ec21-4f02-aa2c-dc116b5576d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3a2e4d3-a535-4850-b6c3-d5072bb19fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truly random shuffling (shuffle=True in PyTorch)\n",
    "# ORDERING = OrderOption.RANDOM\n",
    "\n",
    "# Unshuffled (i.e., served in the order the dataset was written)\n",
    "ORDERING = OrderOption.SEQUENTIAL\n",
    "\n",
    "# Memory-efficient but not truly random loading\n",
    "# Speeds up loading over RANDOM when the whole dataset does not fit in RAM!\n",
    "# ORDERING = OrderOption.QUASI_RANDOM\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 12\n",
    "IMAGE_RESOLUTION = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0e4d00b-3ba3-4c6e-99f4-85fe530b797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Rescale, self).__init__()\n",
    "        self.scale = 255.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d454e1a-afc0-4b40-86ec-00a4348f857f",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 16.0 TiB for an array with shape (732944138157,) and data type [('sample_id', '<u8'), ('ptr', '<u8'), ('size', '<u8')]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m\n\u001b[1;32m     17\u001b[0m PIPELINES \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image_pipeline\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# loader = Loader('/shared_drive/user-files/laion_dataset_200M/ffcv-laion200m-1shard.beton', \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#                 batch_size=BATCH_SIZE,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#                 num_workers=NUM_WORKERS,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#                 order=ORDERING,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#                 pipelines=PIPELINES)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/disks/disk-big2/ffcv-laion200m.beton\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_WORKERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mORDERING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPELINES\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/ffcv/loader/loader.py:134\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, fname, batch_size, num_workers, os_cache, order, distributed, seed, indices, pipelines, custom_fields, drop_last, batches_ahead, recompile)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches_ahead \u001b[38;5;241m=\u001b[39m batches_ahead\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m seed\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader: Reader \u001b[38;5;241m=\u001b[39m \u001b[43mReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_fields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m num_workers\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_last: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m drop_last\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/ffcv/reader.py:15\u001b[0m, in \u001b[0;36mReader.__init__\u001b[0;34m(self, fname, custom_handlers)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_field_descriptors()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_metadata()\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_allocation_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/ffcv/reader.py:67\u001b[0m, in \u001b[0;36mReader.read_allocation_table\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_allocation_table\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     66\u001b[0m     offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malloc_table_ptr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m     alloc_table \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOC_TABLE_TYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     alloc_table\u001b[38;5;241m.\u001b[39msetflags(write\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malloc_table \u001b[38;5;241m=\u001b[39m alloc_table\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 16.0 TiB for an array with shape (732944138157,) and data type [('sample_id', '<u8'), ('ptr', '<u8'), ('size', '<u8')]"
     ]
    }
   ],
   "source": [
    "image_mean = [\n",
    "    0.485,\n",
    "    0.456,\n",
    "    0.406\n",
    "  ]\n",
    "\n",
    "image_std = [\n",
    "    0.229,\n",
    "    0.224,\n",
    "    0.225\n",
    "  ]\n",
    "\n",
    "image_pipeline: List[Operation] = [SimpleRGBImageDecoder(), ToTensor(), Convert(torch.float16), Rescale(), \n",
    "                                   ToDevice(torch.device('cuda'), non_blocking=True), ToTorchImage(),\n",
    "                                   torchvision.transforms.Normalize(image_mean, image_std)]\n",
    "\n",
    "PIPELINES = {\n",
    "  'image': image_pipeline\n",
    "}\n",
    "\n",
    "# loader = Loader('/shared_drive/user-files/laion_dataset_200M/ffcv-laion200m-1shard.beton', \n",
    "#                 batch_size=BATCH_SIZE,\n",
    "#                 num_workers=NUM_WORKERS,\n",
    "#                 order=ORDERING,\n",
    "#                 pipelines=PIPELINES)\n",
    "loader = Loader('/mnt/disks/disk-big2/ffcv-laion200m.beton', \n",
    "                batch_size=BATCH_SIZE,\n",
    "                num_workers=NUM_WORKERS,\n",
    "                order=ORDERING,\n",
    "                pipelines=PIPELINES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51e99a35-fbd9-4f47-96d6-2ad6bc141de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517eb128-4d32-447a-a1e6-a1d9655d851f",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eec1c70-df07-407c-80e6-ea14b2cedfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "\n",
    "model = model.to(device, dtype=torch.float16)\n",
    "\n",
    "target_sizes = torch.tensor([[IMAGE_RESOLUTION, IMAGE_RESOLUTION]]*BATCH_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "793df8ea-e300-4b39-947a-9f1400bb5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_parquet(results, urls, model, step):\n",
    "    rows = []\n",
    "    columns = ['url', 'label', 'score', 'top_left_x', 'top_left_y', 'bottom_right_x', 'bottom_right_y']\n",
    "    for i, result_per_image in enumerate(results):\n",
    "        for score, label, box in zip(result_per_image[\"scores\"], result_per_image[\"labels\"], result_per_image[\"boxes\"]):\n",
    "            url = urls[i]\n",
    "            box = box.detach().cpu()\n",
    "            top_left_x, top_left_y, bottom_right_x, bottom_right_y = box[0].item(), box[1].item(), box[2].item(), box[3].item()\n",
    "            label_name = model.config.id2label[label.item()]\n",
    "            score = np.round(score.detach().cpu().item(), 2)\n",
    "    \n",
    "            row = [url, label_name, score, top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n",
    "            rows.append(row)\n",
    "        \n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df.to_parquet(f'/mnt/disks/disk-big2/laion200m-od-labels-1shard/{step}_batch.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2038e00e-be40-4930-a6a9-7e7671b98387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_object_detection(\n",
    "        outputs, threshold: float = 0.5, target_sizes = None\n",
    "    ):\n",
    "        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n",
    "\n",
    "        if target_sizes is not None:\n",
    "            if len(out_logits) != len(target_sizes):\n",
    "                raise ValueError(\n",
    "                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n",
    "                )\n",
    "\n",
    "        # t = time()\n",
    "        prob = nn.functional.softmax(out_logits, -1)\n",
    "        scores, labels = prob[..., :-1].max(-1)\n",
    "        # print(time() - t)\n",
    "\n",
    "        # t = time()\n",
    "        # Convert to [x0, y0, x1, y1] format\n",
    "        boxes = center_to_corners_format(out_bbox)\n",
    "        # print(time() - t)\n",
    "\n",
    "        mask = scores > threshold\n",
    "    \n",
    "        # mask = mask.cpu()\n",
    "        # scores = scores.cpu()\n",
    "        # labels = labels.cpu()\n",
    "        # boxes = boxes.cpu()\n",
    "\n",
    "        # t = time()\n",
    "        # Convert from relative [0, 1] to absolute [0, height] coordinates\n",
    "        if target_sizes is not None:\n",
    "            if isinstance(target_sizes, List):\n",
    "                img_h = torch.Tensor([i[0] for i in target_sizes])\n",
    "                img_w = torch.Tensor([i[1] for i in target_sizes])\n",
    "            else:\n",
    "                img_h, img_w = target_sizes.unbind(1)\n",
    "\n",
    "            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n",
    "            boxes = boxes * scale_fct[:, None, :]\n",
    "        # print(time() - t)\n",
    "\n",
    "        # t = time()\n",
    "        results = []\n",
    "    \n",
    "        # print(len(scores))\n",
    "        # mask_idx, class_idx = torch.where(mask)\n",
    "        # class_idx[i, i*128]\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "        # for s, l, b in zip(scores, labels, boxes):\n",
    "            score = scores[i][mask[i]]\n",
    "            label = labels[i][mask[i]]\n",
    "            box = boxes[i][mask[i]]\n",
    "            # print(s.shape)\n",
    "            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n",
    "        # print(time() - t)\n",
    "        # print()\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5c4bb81-a0a9-4a48-8413-6796d0640898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = torch.rand(128, 100)\n",
    "# mask = scores > 0.5\n",
    "\n",
    "# coord = torch.nonzero(mask)\n",
    "# torch.where(mask), torch.where(mask)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ecc95-2aee-4dc9-a417-42a24e2e2d65",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269d9cf8-fe25-4505-a385-952cee5c4ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/shared_drive/user-files/ControlNetScalingLaws/wandb/run-20240316_204354-gq58fnpr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/team__1/object-detector/runs/gq58fnpr' target=\"_blank\">ancient-shadow-4</a></strong> to <a href='https://wandb.ai/team__1/object-detector' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/team__1/object-detector' target=\"_blank\">https://wandb.ai/team__1/object-detector</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/team__1/object-detector/runs/gq58fnpr' target=\"_blank\">https://wandb.ai/team__1/object-detector/runs/gq58fnpr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"object-detector\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de981c4f-0efc-4d36-8c76-b605fe376482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ff4458ecf04b41a73b0315bf231ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m batch_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m: batch}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m results \u001b[38;5;241m=\u001b[39m post_process_object_detection(outputs, target_sizes\u001b[38;5;241m=\u001b[39mtarget_sizes, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m     16\u001b[0m save_results_to_parquet(results\u001b[38;5;241m=\u001b[39mresults, \n\u001b[1;32m     17\u001b[0m                             urls\u001b[38;5;241m=\u001b[39mbatch_urls, \n\u001b[1;32m     18\u001b[0m                             model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m     19\u001b[0m                             step\u001b[38;5;241m=\u001b[39mstep)\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py:1564\u001b[0m, in \u001b[0;36mDetrForObjectDetection.forward\u001b[0;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1561\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;66;03m# First, sent images through DETR base model to obtain encoder + decoder outputs\u001b[39;00m\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;66;03m# class logits + predicted bounding boxes\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py:1399\u001b[0m, in \u001b[0;36mDetrModel.forward\u001b[0;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     pixel_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(((batch_size, height, width)), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;66;03m# First, sent pixel_values + pixel_mask through Backbone to obtain the features\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;66;03m# pixel_values should be of shape (batch_size, num_channels, height, width)\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;66;03m# pixel_mask should be of shape (batch_size, height, width)\u001b[39;00m\n\u001b[0;32m-> 1399\u001b[0m features, object_queries_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;66;03m# get final feature map and downsampled mask\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m feature_map, mask \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py:408\u001b[0m, in \u001b[0;36mDetrConvModel.forward\u001b[0;34m(self, pixel_values, pixel_mask)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, pixel_mask):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# send pixel_values and pixel_mask through backbone to get list of (feature_map, pixel_mask) tuples\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     pos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_map, mask \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;66;03m# position encoding\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py:386\u001b[0m, in \u001b[0;36mDetrConvEncoder.forward\u001b[0;34m(self, pixel_values, pixel_mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor, pixel_mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# send pixel_values through the model to get list of feature maps\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(pixel_values) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_timm_backbone \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfeature_maps\n\u001b[1;32m    388\u001b[0m     out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_map \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;66;03m# downsample pixel_mask to match shape of corresponding feature_map\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:494\u001b[0m, in \u001b[0;36mResNetBackbone.forward\u001b[0;34m(self, pixel_values, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    488\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    489\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    492\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(pixel_values)\n\u001b[0;32m--> 494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states\n\u001b[1;32m    498\u001b[0m feature_maps \u001b[38;5;241m=\u001b[39m ()\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:254\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, hidden_state, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    252\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 254\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mstage_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    257\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:223\u001b[0m, in \u001b[0;36mResNetStage.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    221\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 223\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_state\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:182\u001b[0m, in \u001b[0;36mResNetBottleNeckLayer.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[1;32m    181\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_state\n\u001b[0;32m--> 182\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(residual)\n\u001b[1;32m    184\u001b[0m     hidden_state \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:74\u001b[0m, in \u001b[0;36mResNetConvLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 74\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization(hidden_state)\n\u001b[1;32m     76\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(hidden_state)\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared_drive/user-files/miniconda3/envs/ffcv/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "for step, (batch, label) in tqdm(enumerate(loader)):\n",
    "    # try:\n",
    "        label_json = JSONField.unpack(label)\n",
    "        \n",
    "        batch_urls = [dict_['url'] for dict_ in label_json]\n",
    "            \n",
    "        batch_dict = {'pixel_values': batch}\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch_dict)\n",
    "        \n",
    "        results = post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)\n",
    "        \n",
    "        save_results_to_parquet(results=results, \n",
    "                                    urls=batch_urls, \n",
    "                                    model=model, \n",
    "                                    step=step)\n",
    "    #     wandb.log({\"n_batch\": step})\n",
    "    # except:\n",
    "    #     wandb.alert(title=f\"Batch Warning!\",\n",
    "    #                 text=f\"Problem with batch {step}\",\n",
    "    #                 level=AlertLevel.WARN)\n",
    "\n",
    "print(time() - t)\n",
    "# wandb.alert(title=f\"Run finished!\",\n",
    "#             text = f\"Objects successfully detected in all {step+1} batches in {np.round((time() - t)/3600, 2)} hours !!! :)\",\n",
    "#                    level=AlertLevel.INFO)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c437051-5ab5-412e-a7ba-6a24a868dde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9cee945-a2f2-499e-a296-e743df8635fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/mnt/disks/disk-big2/laion200m-od-labels-1shard/0_batch.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf2e4f22-8380-4b6f-b03d-c06d05b1b106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj6BI/AAAAAAAAasE/uT9KjqByvLQ/s330/%25E8%25A8%2598%25E4%25BA%258B%25E6%259C%25AC_121214_193308_1.jpg'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e57cbff-59fe-40a0-a11c-66936b8ff408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>top_left_x</th>\n",
       "      <th>top_left_y</th>\n",
       "      <th>bottom_right_x</th>\n",
       "      <th>bottom_right_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://t0.gstatic.com/images?q=tbn:ANd9GcQsZJi...</td>\n",
       "      <td>car</td>\n",
       "      <td>1.00</td>\n",
       "      <td>56.656250</td>\n",
       "      <td>137.5000</td>\n",
       "      <td>152.00000</td>\n",
       "      <td>164.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj...</td>\n",
       "      <td>clock</td>\n",
       "      <td>1.00</td>\n",
       "      <td>118.812500</td>\n",
       "      <td>141.8750</td>\n",
       "      <td>140.12500</td>\n",
       "      <td>160.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj...</td>\n",
       "      <td>person</td>\n",
       "      <td>0.90</td>\n",
       "      <td>124.875000</td>\n",
       "      <td>221.1250</td>\n",
       "      <td>137.87500</td>\n",
       "      <td>246.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://st.depositphotos.com/3336339/4632/i/170...</td>\n",
       "      <td>tv</td>\n",
       "      <td>0.99</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>21.0625</td>\n",
       "      <td>230.50000</td>\n",
       "      <td>205.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://img.deanscards.com/thumb/1357368b.jpg</td>\n",
       "      <td>book</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>43.2500</td>\n",
       "      <td>255.37500</td>\n",
       "      <td>219.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://cdn11.bigcommerce.com/s-1mxugrbmxo/ima...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>1.00</td>\n",
       "      <td>21.875000</td>\n",
       "      <td>39.6875</td>\n",
       "      <td>236.12500</td>\n",
       "      <td>221.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>http://v.fashionov.com/products/small/122421/2...</td>\n",
       "      <td>person</td>\n",
       "      <td>1.00</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>151.7500</td>\n",
       "      <td>230.75000</td>\n",
       "      <td>249.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>http://media.rightmove.co.uk/dir/10k/9111/5323...</td>\n",
       "      <td>car</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>173.3750</td>\n",
       "      <td>31.46875</td>\n",
       "      <td>226.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>http://media.rightmove.co.uk/dir/10k/9111/5323...</td>\n",
       "      <td>car</td>\n",
       "      <td>0.99</td>\n",
       "      <td>193.500000</td>\n",
       "      <td>168.2500</td>\n",
       "      <td>256.00000</td>\n",
       "      <td>230.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>https://african-import.com/wp-content/uploads/...</td>\n",
       "      <td>person</td>\n",
       "      <td>1.00</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>39.0000</td>\n",
       "      <td>230.00000</td>\n",
       "      <td>256.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url   label  score  \\\n",
       "0    http://t0.gstatic.com/images?q=tbn:ANd9GcQsZJi...     car   1.00   \n",
       "1    http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj...   clock   1.00   \n",
       "2    http://4.bp.blogspot.com/-VB3VQfCFnng/VIu0_zQj...  person   0.90   \n",
       "3    http://st.depositphotos.com/3336339/4632/i/170...      tv   0.99   \n",
       "4         http://img.deanscards.com/thumb/1357368b.jpg    book   0.90   \n",
       "..                                                 ...     ...    ...   \n",
       "168  https://cdn11.bigcommerce.com/s-1mxugrbmxo/ima...  laptop   1.00   \n",
       "169  http://v.fashionov.com/products/small/122421/2...  person   1.00   \n",
       "170  http://media.rightmove.co.uk/dir/10k/9111/5323...     car   0.99   \n",
       "171  http://media.rightmove.co.uk/dir/10k/9111/5323...     car   0.99   \n",
       "172  https://african-import.com/wp-content/uploads/...  person   1.00   \n",
       "\n",
       "     top_left_x  top_left_y  bottom_right_x  bottom_right_y  \n",
       "0     56.656250    137.5000       152.00000         164.250  \n",
       "1    118.812500    141.8750       140.12500         160.875  \n",
       "2    124.875000    221.1250       137.87500         246.875  \n",
       "3     18.000000     21.0625       230.50000         205.250  \n",
       "4      0.875000     43.2500       255.37500         219.250  \n",
       "..          ...         ...             ...             ...  \n",
       "168   21.875000     39.6875       236.12500         221.750  \n",
       "169  175.000000    151.7500       230.75000         249.750  \n",
       "170    0.023438    173.3750        31.46875         226.375  \n",
       "171  193.500000    168.2500       256.00000         230.000  \n",
       "172   57.000000     39.0000       230.00000         256.000  \n",
       "\n",
       "[173 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d618ff-5e78-40fd-9e30-fc42264489ae",
   "metadata": {},
   "source": [
    "10 batches (128 images) - 1 sec (FFCV loader)\\\n",
    "5 batch (128 images) - 1 sec (HF loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e12018-acd7-4928-af6b-fd59ba66a4ba",
   "metadata": {},
   "source": [
    "# HF dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea20c7a-4287-4241-b3ca-bf05b427895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79804925-d565-41d4-a070-dc246e39ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = '/shared_drive/user-files/laion_dataset_200M/laion200m-data/0-12_2M/'\n",
    "\n",
    "my_shards = glob(path.join(FOLDER, '*'))\n",
    "tar_my_shards = [file for file in my_shards if file.endswith('.tar')][:1]\n",
    "\n",
    "seed=42 \n",
    "buffer_size=100\n",
    "\n",
    "iterable_dataset = load_dataset(\"webdataset\", data_files={\"train\": tar_my_shards}, split=\"train\", \n",
    "                       streaming=True)\n",
    "\n",
    "def get_url(example):\n",
    "        url = example['json']['url']\n",
    "        example['url'] = url\n",
    "        return example\n",
    "    \n",
    "iterable_dataset = iterable_dataset.map(get_url)\n",
    "train_dataset = iterable_dataset\n",
    "# .shuffle(seed=seed, buffer_size=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91f42d3f-2cd7-48af-b626-9a6df099f9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/shared_drive/user-files/laion_dataset_200M/laion200m-data/0-12_2M/00682.tar']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_my_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c7cc344-b178-4356-bd47-4516ba3a6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(train_dataset): \n",
    "    images = []\n",
    "    urls = []\n",
    "    \n",
    "    for example in train_dataset:\n",
    "        images.append(example['jpg'].convert(\"RGB\"))\n",
    "        urls.append(example['url'])\n",
    "\n",
    "    processed = detr_processor(images=images, return_tensors=\"pt\", do_resize=False)\n",
    "    # print(detr_processor)\n",
    "    return processed, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c08685c-eca0-4802-b709-d958b67a7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "dataloader = DataLoader(train_dataset, \n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b866ae09-441c-42c4-b9d2-b48688787f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c3fcb05e2541c9b826a0c9d1d5bfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1469033/170280622.py\u001b[0m(2)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 2 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m    \u001b[0;31m# print(batch['pixel_values'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m    \u001b[0mbatch_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0mbatch_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][0].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][0].min()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][0].max()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'][10].min()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'].min()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb>  batch_tensors['pixel_mask'].max()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "for step, batch in tqdm(enumerate(dataloader)):\n",
    "    # print(batch['pixel_values'].shape)\n",
    "    batch_urls = batch[1]\n",
    "    batch_tensors = batch[0]\n",
    "    pdb.set_trace()\n",
    "    \n",
    "print(time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c7808-55e1-4bdc-9204-0f7301ca182c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059104c-cf07-46d3-a8ec-4228408aca34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
